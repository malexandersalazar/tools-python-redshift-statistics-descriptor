{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import appsettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER = appsettings.SERVER\n",
    "USER = appsettings.USER\n",
    "PASSWORD = appsettings.PASSWORD\n",
    "DRIVER = '{Amazon Redshift ODBC Driver (x64)}'\n",
    "DATABASE = 'dev'\n",
    "SCHEMA = 'public'\n",
    "ROWS = 10000\n",
    "LEVEL = 't'\n",
    "TABLE = 'category'\n",
    "ASSOCIATIONS = False\n",
    "OPEN_BROWSER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import sweetviz as sv\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection():\n",
    "    return pyodbc.connect('DRIVER='+DRIVER+';SERVER='+SERVER+';PORT=5439;DATABASE='+DATABASE+';UID='+USER+';PWD='+PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = []\n",
    "\n",
    "if(LEVEL=='s'):\n",
    "    with create_connection() as conn1:\n",
    "        with conn1.cursor() as schema_cur:\n",
    "            schema_cur.execute('select \"table\", size, pct_used, tbl_rows from svv_table_info')\n",
    "            schema_row = schema_cur.fetchone()\n",
    "            while schema_row:\n",
    "                table_name = str(schema_row[0])\n",
    "                table_size = str(schema_row[1])\n",
    "                table_pct_used = str(schema_row[2])\n",
    "                table_rows = str(schema_row[3])\n",
    "\n",
    "                tables.append((f'{SCHEMA}.{table_name}',table_size,table_pct_used,table_rows))\n",
    "                schema_row = schema_cur.fetchone()\n",
    "else:\n",
    "    with create_connection() as conn1:\n",
    "        with conn1.cursor() as table_cur:\n",
    "            table_cur.execute(f\"select size, pct_used, tbl_rows from svv_table_info where svv_table_info.table='{TABLE}'\")\n",
    "            table_row = table_cur.fetchone()\n",
    "            \n",
    "            table_size = str(table_row[0])\n",
    "            table_pct_used = str(table_row[1])\n",
    "            table_rows = str(table_row[2])\n",
    "\n",
    "            tables.append((f'{SCHEMA}.{TABLE}',table_size,table_pct_used,table_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('obj'):\n",
    "    os.mkdir('obj')\n",
    "\n",
    "info_arr = []\n",
    "\n",
    "for (table_name,table_size,table_pct_used,table_rows) in tables:\n",
    "    print(f'Loading {table_name}...')\n",
    "\n",
    "    sheet_name = table_name.split('.')[-1]\n",
    "\n",
    "    sample_query = f'SELECT * FROM {table_name} ORDER BY RANDOM() LIMIT {ROWS}'\n",
    "    with create_connection() as conn3:\n",
    "        sample_cur = conn3.cursor().execute(sample_query)\n",
    "        sample_df = pd.DataFrame.from_records(\n",
    "            iter(sample_cur), columns=[x[0] for x in sample_cur.description])\n",
    "\n",
    "        info_arr.append([table_name,table_size,table_pct_used,table_rows, len(sample_df)])\n",
    "\n",
    "        analysis = sv.analyze(sample_df, pairwise_analysis=(\"on\" if ASSOCIATIONS else \"off\"))\n",
    "        analysis.show_html(f'obj/{sheet_name}.html', open_browser=OPEN_BROWSER)\n",
    "\n",
    "    del sample_df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_info_df = pd.DataFrame(info_arr, columns = ['TABLE NAME','TABLE SIZE (MB)','TABLE PCT. USED','TABLE ROWS','SAMPLE ROWS'])\n",
    "excel_writer = pd.ExcelWriter(f'obj/{SCHEMA}_EDA_INFO.xlsx', engine='xlsxwriter')\n",
    "eda_info_df.to_excel(excel_writer, sheet_name=SCHEMA, index=False)\n",
    "worksheet = excel_writer.sheets[SCHEMA]\n",
    "for idx, col in enumerate(eda_info_df):  # Loop through all columns\n",
    "    series = eda_info_df[col]\n",
    "    max_len = max((\n",
    "        series.astype(str).str.len().max(),  # Len of largest item\n",
    "        len(str(series.name))  # Len of column name/header\n",
    "        )) + 9  # Adding a little extra space\n",
    "    worksheet.set_column(idx, idx, max_len)  # Set column width\n",
    "excel_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datafabric-dev-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06fd14dd0464832b5d48561070e33a75f30377a884f82ade456f68b55551612f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
